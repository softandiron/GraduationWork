## Дипломная работа в Нетологии
### По курсу: Продвинутый инжиниринг данных
### Работу выполнил: Минин Артём
#
### Задание:

Вам необходимо построить airflow пайплайн выгрузки ежедневных отчётов по количеству поездок на велосипедах в городе Нью-Йорк.

Рекомендации при выполнении работы:

Пайплайн должен состоять из следующих шагов:

Отслеживание появление новых файлов в своём бакете на AWS S3. Представим, что пользователь или провайдер данных будет загружать новые исторические данные по поездкам в Ваш бакет;
При появлении нового файла запускается оператор импорта данных в созданную таблицу базы данных Clickhouse;
Необходимо сформировать таблицы с ежедневными отчётами по следующим критериям:
– количество поездок в день
– средняя продолжительность поездок в день
– распределение поездок пользователей, разбитых по категории «gender»
Данные статистики необходимо загрузить на специальный S3 бакет с хранящимися отчётами по загруженным файлам.

### Решение:
В ходе выполнения выпускной работы на виртуальную машину с операционной системой Linux Mint
был установлен Apache Airflow и Scheduler.
Так же был создан аккаунт Amazon AWS S3 с двумя бакетами: 'netobucket' и 'netobucketreports'.
В первом хранятся исходные данные (появляющиеся данные по поездкам),
а во второй бакет должны загружаться готовые отчёты.

Для решения задачи на той же виртуальной машине был запущен сервер Clickhouse.
В качестве редактора кода была установленна IDE PyCharm.

В IDE был создан DAG, который после нескольких переделок получил название S3_ETL_v3.py

Изначально в указанном даге пайплайн был реализован только одна задачей, представляющей собой
функцию типа PythonOperator. В дальнейшем, уже после сдачи работы, был проведён глубокий 
рефакторинг проекта, в ходе которого пайплайн оказался разбит на два таска, представленных на рисунке 1.

![Рисунок 1. Задачи.](https://github.com/softandiron/GraduationWork/blob/ca63697243afacf360728a18e83f229d714cf364/screenshots/Screenshot%20from%202022-05-30%2000-24-32.png)

В первом таске `download_files` выполняются следующие действия:
- Получение пользовательских данных из файла `config.py`;
- Подключение к бакету с исходными файлами;
- Получение списка файлов в исходном бакете;
- Чтение файла `old_files.csv` с получением из него списка ранее обработанных файлов;
- Выявление и скачивание новых файлов из исходного бакета на локальную машину;
- Проверка скаченного архива и извлечение скаченного файла в формате .CSV в локальную 
директорию `/datafiles`, удаление архива.

Результатом работы первого таска является получение одного или более файла с исходными данными
по поездкам за месяц в формате .CSV при наличии новых архивов на исходном бакете.

Во втором таске `get_and_load_reports` выполняются следующие действия:
- Получение пользовательских данных из файла `config.py`;
- Подключение к бакету с готовыми отчётами;
- Создание таблицы `trips` в базе данных Clickhouse;
- Поиск скаченных .CSV файлов с исходными данными в директории `/datafiles`;
- Из каждого файла проводится формирование объекта Pandas Dataframe;
- Из каждого сформированного Pandas Dataframe отбираются необходимые колонки с данными и
загружаются в таблицу `trips` в Clickhouse;
- С помощью SQL запросов к базе Clickhouse из таблицы формируются отчёты по поездкам
в соответствии с заданием. Отчёты попадают в новый Dataframe, после чего таблица очищается;
- Dataframe с готовыми отчётами по поездкам превращется в .CSV файл, который загружается
на бакет с готовыми отчётами;
- После обработки файлы с исходными данными в директории `/dafailes` удаляются.

Результатом работы второго таска является один или более отчёт в формате .csv загруженный
в целевой бакет, при наличии исходных файлов в результате работы первого таска.

![Рисунок 2. Готовые отчёты в целевом бакете](https://github.com/softandiron/GraduationWork/blob/ca63697243afacf360728a18e83f229d714cf364/screenshots/Screenshot%20from%202022-05-30%2000-29-28.png)

В процессе работы пайплайна предусмотренно подробное логирование каждого шага выполнения программы.
Логи можно посмотреть после завершения выполнения задачи в интерфейсе Apache Airflow.

![Рисунок 3. Логи](https://github.com/softandiron/GraduationWork/blob/ca63697243afacf360728a18e83f229d714cf364/screenshots/Screenshot%20from%202022-05-30%2000-27-28.png)

Время выполнения ETL процесса сильно варьируется в зависимости от наличия новых архивов 
в исходном бакете. В моём случае полная обработка 5 новых файлов заняла 56 секунд.
При этом, время на выполнение первого и второго тасков распределилось примерно поровну,
выполнение первого таска оказалось лишь немного больше.
Выполнение программы в ситуации, когда на исходном бакете оказываются только старые файлы,
занимало 9 секунд.

![Рисунок 4. Сетка](https://github.com/softandiron/GraduationWork/blob/ca63697243afacf360728a18e83f229d714cf364/screenshots/Screenshot%20from%202022-05-30%2000-26-05.png)

В первой версии программы список ранее обработанных файлов хранился в Apache Airflow
Variables, однако в последствии было принято решение использовать для этого локальный 
файл `old_files.csv`.

Общая схема работы пайплайна представлена на следующем рисунке.

![Рисунок 5. My ETL scheme](https://github.com/softandiron/GraduationWork/blob/main/My%20ETL.jpeg)


### Как пользоваться

Для работы программы должен быть установлен Apache Airflow. Worker и scheduler запускаются в папке с
данным проектом, либо можно копировать файлы проекта в папку с Airflow.
Мануалы по установке и запуску имеются в папке `manuals` данного репозитория. 

Кроме этого должен быть установлен и запущен сервер с базой данных Clickhouse.

Для загрузки целевых отчётов должен быть соответсвующий бакет AWS S3 и ключи для доступа к нему.


Для начала работы нужно создать в основной директории программы файл `config.py`
В файле прописать необходимые данные в соответствии с шаблоном:


`DB_NAME = 'tripDB'`

`HOST = 'http://localhost:8123'`

`USER = 'default'`

`PASSWORD = ''`

`FROM_BUCKET = 'netobucket'`

`TO_BUCKET = 'netobucketreports'`

`ACCESS_KEY = 'long key'`

`SECRET_KEY = 'another long key'`

После этого осталось открыть интерфейс Airflow и запустить DAG `S3_ETL_v3`
